# Adaptive_Attention_Span_in_Transformers
Implement and evaluate adaptive attention span, where each attention head learns its own optimal span through a parameterized soft masking function, improving efficiency without sacrificing performance.
