
AdaptiveMask es el componente fundamental del mecanismo de adaptive attention span. Esta clase implementa una funciÃ³n de enmascaramiento suave (soft masking) que permite aprender de manera diferenciable cuÃ¡ntos tokens del pasado deberÃ­a atender cada cabeza de atenciÃ³n.
Concepto clave: En lugar de usar un span fijo (ej: siempre ver los Ãºltimos 512 tokens), esta mÃ¡scara aprende un parÃ¡metro z que determina dinÃ¡micamente cuÃ¡ntos tokens son relevantes. La transiciÃ³n de "atender" a "no atender" es gradual (suave), lo que permite el aprendizaje mediante backpropagation.

**PropÃ³sito**: Aplica la mÃ¡scara suave al tensor de entrada (tÃ­picamente pesos de atenciÃ³n).

**Proceso paso a paso**:

1. **mask = self.mask_template + self.current_val * self._max_size`**:
   - self.current_val estÃ¡ en [0, 1], lo multiplicamos por max_size para obtener el span actual
   - Si current_val = 0.5 y max_size = 100, entonces el span efectivo es 50
   - Sumamos a la plantilla: transladamos la funciÃ³n

2. **`mask = mask / self._ramp_size + 1`**:
   - Dividimos por `ramp_size` para controlar la pendiente de la transiciÃ³n
   - Sumamos 1 para centrar la funciÃ³n

3. **`mask = mask.clamp(0, 1)`**:
   - Limitamos los valores entre 0 y 1
   - Valores fuera del span efectivo â†’ 0
   - Valores dentro del span efectivo â†’ 1
   - Valores en la rampa â†’ entre 0 y 1 (transiciÃ³n suave)

4. **Recorte condicional**:
   - Si la entrada es mÃ¡s pequeÃ±a que `max_size`, ajustamos la mÃ¡scara para que coincida

5. **`x = x * mask`**:
   - Multiplicamos elemento a elemento: enmascaramos la entrada

**Ejemplo visual**:

Supongamos max_size=10, ramp_size=2, current_val=0.6 (span=6)

Posiciones:     -9  -8  -7  -6  -5  -4  -3  -2  -1   0
Mask:            0   0   0   0  0.5  1   1   1   1   1


AdaptiveSpan es la clase que orquesta el mecanismo completo de adaptive attention span para un Transformer. Mientras que AdaptiveMask es el componente bÃ¡sico de enmascaramiento, AdaptiveSpan lo integra en el contexto de la atenciÃ³n multi-cabeza, gestionando:

MÃºltiples mÃ¡scaras (una por cabeza de atenciÃ³n)
RegularizaciÃ³n del span mediante una pÃ©rdida auxiliar
OptimizaciÃ³n de memoria mediante cache adaptativo
Recorte de memoria innecesaria para reducir cÃ³mputo

**Flujo visual**:

Input:  (B*H, M, L)  [batch*heads fusionados]
   â†“
Reshape: (B, H, M, L)  [separar batch y heads]
   â†“
Mask:    (B, H, M, L)  [aplicar mÃ¡scara por cabeza]
   â†“
Normalize: (B, H, M, L)  [re-normalizar]
   â†“
Reshape: (B*H, M, L)  [fusionar de nuevo]


RegularizaciÃ³n L1 para penalizar spans grandes.

L_span = Î» * Î£ z_i
donde z_i es el span de la cabeza i.
loss = Î» * max_span * mean(current_val)

Por quÃ© es importante:

Sin regularizaciÃ³n, todas las cabezas tenderÃ­an a usar el span mÃ¡ximo
Esta pÃ©rdida incentiva spans pequeÃ±os (eficiencia)
El modelo encuentra el balance Ã³ptimo entre:

Span grande: mejor rendimiento pero mÃ¡s cÃ³mputo

## Diagrama de flujo completo
InicializaciÃ³n:
  AdaptiveSpan crea AdaptiveMask con (nb_heads, 1, 1) parÃ¡metros
  Cada cabeza tiene su propio z âˆˆ [0, 1]
  
Durante Forward:
  1. trim_memory() â†’ recorta cache innecesario
  2. [cÃ³mputo de atenciÃ³n en models.py]
  3. forward() â†’ aplica mÃ¡scaras adaptativas
  4. Cada cabeza enmascara segÃºn su span aprendido
  
Durante Backward:
  1. get_loss() â†’ calcula penalizaciÃ³n L1
  2. Gradientes fluyen a travÃ©s de current_val
  3. optimizer.step() â†’ actualiza spans
  4. clamp_param() â†’ asegura z âˆˆ [0, 1]
  
GestiÃ³n de memoria:
  - get_cache_size() â†’ determina memoria a reservar
  - get_trim_len() â†’ optimiza cÃ³mputo


Clase SeqAttention(nn.Module)
Esta es la clase mÃ¡s importante - donde ocurre la magia de la atenciÃ³n adaptativa.
Resumen General
SeqAttention implementa self-attention secuencial con span adaptativo. CaracterÃ­sticas clave:

Atiende solo a tokens anteriores (autoregresivo)
Cada token atiende a un nÃºmero variable y aprendible de pasos previos
Usa position embeddings relativos para capturar orden
Opcionalmente integra persistent memory (para el paper de all-attention networks)


Resumen de SeqAttention
Input: query (tokens actuales), key/value (historia + actuales), key_pe

1. [Opcional] trim_memory() â†’ Optimizar memoria
2. attn_content = query Â· key â†’ AtenciÃ³n por contenido
3. attn_pos = query Â· key_pe â†’ AtenciÃ³n por posiciÃ³n
4. attn = attn_content + attn_pos â†’ Combinar
5. attn = softmax(attn / âˆšH) â†’ Normalizar
6. [ADAPTIVE SPAN ] attn = adaptive_span(attn)
7. attn = dropout(attn) â†’ Regularizar
8. output = attn Â· value â†’ Aplicar atenciÃ³n

Output: representaciones enriquecidas (B, M, H)


## Diagrama Completo del Forward

Input: h (B, M, H), h_cache (B, L, H), key_pe

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Paso 1: Concatenar                  â”‚
â”‚ h_all = [h_cache | h]               â”‚
â”‚ (B, L+M, H)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Paso 2: Multi-Head Attention        â”‚
â”‚ attn_out = attn(h, h_all, h_all)   â”‚
â”‚ (B, M, H)                           â”‚
â”‚                                     â”‚
â”‚ Dentro: â­ Adaptive Span â­         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Paso 3: Residual + LayerNorm        â”‚
â”‚ h = norm1(h + attn_out)             â”‚
â”‚                                     â”‚
â”‚     h â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚     â†“       â†“                       â”‚
â”‚   attn    (+)                       â”‚
â”‚     â†“       â†“                       â”‚
â”‚  attn_out  norm1                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Paso 4: FeedForward                 â”‚
â”‚ ff_out = ff(h)                      â”‚
â”‚ (B, M, H)                           â”‚
â”‚                                     â”‚
â”‚ H â†’ 4H â†’ H con ReLU                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Paso 5: Residual + LayerNorm        â”‚
â”‚ out = norm2(h + ff_out)             â”‚
â”‚                                     â”‚
â”‚     h â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚     â†“       â†“                       â”‚
â”‚    ff      (+)                      â”‚
â”‚     â†“       â†“                       â”‚
â”‚  ff_out   norm2                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
      Output (B, M, H)




TransformerSeq es el modelo completo end-to-end que:

Convierte tokens (IDs) en embeddings
AÃ±ade position embeddings
Procesa a travÃ©s de N capas de Transformer (con adaptive span)
Genera probabilidades de siguiente token
Maneja el cache para procesamiento secuencial eficiente

Es lo que entrenaremos y usaremos para generaciÃ³n de texto.

## Flujo Completo del Forward

Input:
  x: (2, 512)           # Token IDs
  h_cache: [12 tensors] # Caches (uno por capa)
  target: (2, 512)      # Targets (opcional)

â†“ [Embedding]
h = in_emb(x)
h: (2, 512, 512)        # Vectores densos

â†“ [Capa 0]
h = layer_0(h, h_cache[0], key_pe)
h_cache_next[0] = actualizar_cache(h, h_cache[0])

â†“ [Capa 1]
h = layer_1(h, h_cache[1], key_pe)
h_cache_next[1] = actualizar_cache(h, h_cache[1])

...

â†“ [Capa 11]
h = layer_11(h, h_cache[11], key_pe)
h_cache_next[11] = actualizar_cache(h, h_cache[11])

â†“ [Output]
logits = out_emb(h)     # (2, 512, 27)
out = log_softmax(logits, dim=-1)

â†“ [Return]
return out, h_cache_next, None


### Flujo Completo

main.py
  â””â”€> train_iteration(eval_only=False, nb_batches=1000)
       â”‚
       â”œâ”€> Loop: 1000 batches
       â”‚    â”‚
       â”‚    â””â”€> _train_batch()
       â”‚         â”‚
       â”‚         â”œâ”€> optimizer.zero_grad()
       â”‚         â”‚
       â”‚         â””â”€> _train_step()
       â”‚              â”‚
       â”‚              â”œâ”€> model.forward(X, h_cache, target=Y)
       â”‚              â”‚    â””â”€> Devuelve: (out, h_cache_next, dummy_loss)
       â”‚              â”‚
       â”‚              â”œâ”€> Calcula loss principal
       â”‚              â”‚
       â”‚              â”œâ”€> loss += adaptive_span_loss  â† SOLO EN TRAIN
       â”‚              â”‚    â””â”€> Penaliza spans largos
       â”‚              â”‚
       â”‚              â””â”€> loss.backward()  â† GRADIENTES
       â”‚
       â””â”€> optimizer.step()  â† ACTUALIZA PESOS
       â””â”€> adaptive_span.clamp_param()  â† z âˆˆ [0,1]


### ðŸŽ¯ RESUMEN CONCEPTUAL

 **Training:**

Procesar bloque â†’ Calcular loss â†’ Agregar adaptive_span_loss â†’ 
Backward â†’ Actualizar pesos â†’ Clamp parÃ¡metros â†’ Siguiente bloque


 **Eval (rÃ¡pido):**
Procesar 10% de bloques â†’ Calcular loss â†’ 
Log para monitorear â†’ (NO tocar pesos)


### Data

## ðŸ“Š FLUJO COMPLETO DE DATOS

1. get_train_val_test_data()
    â†“
2. _build_corpus()
    â†“
3. Dictionary('train.txt', sort_dict)
    - Construye vocabulario
    â†“
4. _tokenize() para train/val/test
    - Convierte texto â†’ Ã­ndices
    â†“
5. _batchify()
    - Reorganiza: (seq_len,) â†’ (batch_size, seq_per_batch)
    â†“
6. DivisiÃ³n para distributed (si aplica)
    - Cada GPU toma su slice
    â†“
7. .to(device)
    - Mover a GPU
    â†“
8. Retornar tensors listos para entrenamiento


## ðŸŽ¯ FLUJO COMPLETO RESUMIDO

1. Parse argumentos (config.py)
    â†“
2. Setup environment (GPUs, distributed)
    â†“
3. Load data (tokenize, batchify, to device)
    â†“
4. Create model (TransformerSeq con adaptive span)
    â†“
5. Wrap con DistributedDataParallel
    â†“
6. Create optimizer (Adagrad) y scheduler (warmup)
    â†“
7. Create logger
    â†“
8. Load checkpoint (si existe)
    â†“
9. Initialize cache (zeros)
    â†“
10. Training loop:
    â”œâ”€> Train iteration (1000 batches)
    â”‚   â”œâ”€> Forward
    â”‚   â”œâ”€> Backward
    â”‚   â”œâ”€> Optimizer step
    â”‚   â””â”€> Update cache
    â”‚
    â”œâ”€> Val iteration (100 batches)
    â”‚   â”œâ”€> Forward (sin backward)
    â”‚   â””â”€> Update cache
    â”‚
    â”œâ”€> Aggregate results (distributed)
    â”œâ”€> Log metrics
    â””â”€> Save checkpoint
    â†“
11. (Opcional) Full eval en test set


### FLOPS

Los FLOPS (Floating Point Operations) son operaciones matemÃ¡ticas de punto flotante que una computadora realiza. En el contexto de redes neuronales, se usan para medir el costo computacional de un modelo.
Â¿QuÃ© son los FLOPS?
Son operaciones bÃ¡sicas como:

Sumas
Multiplicaciones
Divisiones
Exponenciales

Por ejemplo, multiplicar dos matrices de tamaÃ±o n x n requiere aproximadamente 2nÂ³ FLOPS (nÂ³ multiplicaciones + nÂ³ sumas).
Â¿CÃ³mo se calculan en Transformers?
En el paper, los FLOPS se calculan principalmente para dos componentes:
1. Self-Attention Layer
Para cada token en una secuencia de longitud M con span S:

Query-Key similarity: 2 Ã— M Ã— S Ã— d FLOPS

M queries Ã— S keys Ã— d dimensiones Ã— 2 (mult + suma)


Attention-Value: 2 Ã— M Ã— S Ã— d FLOPS
Total por cabeza: 4 Ã— M Ã— S Ã— d
Total con H cabezas: 4 Ã— M Ã— S Ã— d Ã— H

2. Feed-Forward Layer

Primera capa lineal: 2 Ã— M Ã— d Ã— d_ff
Segunda capa lineal: 2 Ã— M Ã— d_ff Ã— d
Total: 4 Ã— M Ã— d Ã— d_ff


